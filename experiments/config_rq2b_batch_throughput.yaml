# RQ2b: Batch Throughput
# Measures offline throughput (TPS) at increasing batch sizes
# Run on each GPU separately: CUDA_VISIBLE_DEVICES=0 python launch_benchmark.py config_rq2b_batch_throughput.yaml

backend: [vllm, sglang, tgi, lmdeploy]

hf_model:
  - name: mistralai/Mistral-7B-Instruct-v0.3
    top_p: 0.95
    max_tokens: 128
    temperature: 0.1
  - name: Qwen/Qwen2.5-7B-Instruct
    top_p: 0.95
    max_tokens: 128
    temperature: 0.1
  - name: meta-llama/Llama-3.1-8B-Instruct
    top_p: 0.95
    max_tokens: 128
    temperature: 0.1

task: [mmlu, qa, summarization, sql]

scenario: batch

batch_size: [16, 32, 64, 128]